\chapter{Rover Implementation}
\lhead{\emph{Rover Implementation}}
\label{chapter:rover}

Due to the rover simply being a test platform, its hardware is fairly simple. It consists of a chassis containing batteries, a power distribution board, motor drivers and motors used to drive the rover's wheels, with the central computer (Raspberry Pi 3 \cite{pi}), gimbal, and gimbal control circuitry all mounted on top (Figure \ref{fig:internals}, \ref{fig:hardware}). A detailed breakdown can be found in Appendix \ref{appendix:hardware}. The application of computer vision techniques in an embedded system has high processing requirements, leading to the selection of a Raspberry Pi 3 as the core of the system (it was the highest performance embedded device readily available). 

\begin{figure}[H]
    \begin{center}
    \begin{tabular}{ c }
        \includegraphics[width=0.9\textwidth]{Figures/RoverAbove.jpg} \\
        \includegraphics[width=0.9\textwidth]{Figures/RoverInternals.jpg}
    \end{tabular}
    \caption[Rover Internals]{Rover Internals. The top picture is a top-down view of rover, in which you can see the Pi, gimbal control circuitry, and gimbal. The bottom picture is of the inside of the rover, taken through a hatch in its underside. In this picture you can see velcro strips marking where the batteries are mounted, power distribution board, safety fuse, 2 DC motor drivers, and 2 of the 4 DC motors.}
    \label{fig:internals}
    \end{center}
\end{figure}

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=0.9\textwidth]{Figures/hardware.png}
      \caption[Hardware Block Diagram]{Hardware Block Diagram. The red blocks are motors, green blocks control system components, and blue blocks parts of the image pipeline.}
      \label{fig:hardware}
    \end{center}
\end{figure}

\section{Gimbal Design}

The choice of cameras had to fulfil a very specific set of requirements. The two cameras must be same, as any differences in the images due to using different lenses or sensors would reduce the quality of depth map produced on the server. This makes the most obvious camera choice, the R-Pi camera module, unusable, as the Pi cannot interface with two simultaneously. The two cameras must also be able to take pictures on command from the Pi with low latency, reducing the possible options down to primarily USB webcams. Finally, they must have a high shutter speed. Any motion blur in the images will blur all the edges they contain, making them undetectable by the edge detection algorithm, and any morphing of the image while under motion due to the time it takes the shutter to pass across the entire sensor will once again reduce the quality of the depth map. A high shutter speed reduces motion blur and shutter related morphing, therefore making it essential for whenever the rover is in motion. This requirement reduces the possible cameras down to primarily dedicated computer vision cameras, however these are expensive and often too large to build a gimbal for without also buying expensive motors.

Drone FPV cameras were tested as an alternative, their low latency and use of a global shutter being ideal for this system, however the analogue to digital converters required to interface them with the Pi were too unreliable and low performance to make them a viable option. Only one camera was found that fulfilled all the requirements listed whilst also being reasonably priced and reliable- the PlayStation 3 (PS3) Eye. The PS3 Eye is a peripheral for the PS3 that facilitates games that incorporate aspects of computer vision, so it is designed with computer vision and value for money in mind. While the image quality is fairly poor, this does not significantly impact the quality of the abstractions the system produces due to most detail being discarded as part of the process. The only real flaw in using the PS3 Eye in this system is its poor dynamic range; if the camera is aimed at a window during the day, all the colours in the room will become unrecognisably dark. However, the PS3 Eye has such impressive shutter speed and latency for the price point that this deficiency is an acceptable trade off.

The design of the gimbal (Figure \ref{fig:gimble}) has considerable impact on the 3D model the system generates. As mentioned in Section \ref{subsection:depth}, the images must be rectified to account for discrepancies in the mounting before the depth map is generated on the server, therefore the closer the cameras are to parallel with each other, the less severe the rectification applied must be. Similarly the stability of the gimbal is very important, as any vibrations will cause inconsistency in the alignment of the cameras, leading to inaccurate depth maps. This led to the chosen design where the X-axis (tilt) motor is located centrally, between the cameras, to balance the weight around the rotational axis of the y-axis (rotation) motor. The 3D-printed part the cameras are attached to is also stabilised through mountings on both sides of the x-axis motor, using a ball bearing on the side not driven by the motor. 

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=0.9\textwidth]{Figures/GimbleLabel.jpg}
      \caption[Gimbal Picture]{Gimbal Picture. The Y-axis motor is housed at the bottom, with the X-axis motor directly above it. The ball bearing that stabilises the non-driven side of the cameras' backplate can be seen to the left of the X-axis motor.}
      \label{fig:gimble}
    \end{center}
\end{figure}

Another important aspect of the gimbal design is the distance between the camera lenses. The further apart the two cameras are, the closer distance objects will be in the depth map. Ideally we would aim to match the interpupillary distance of human eyes (63mm on average \cite{dodgson2004variation}), so objects in the 3D environment appear as close as they would were the user standing in the place of the robot. However, with the X-axis motor located centrally it is not possible to mount the cameras that close together. The inter-lens distance in the final design is 120mm, as this is the closest distance possible without reducing the stability of the gimbal. While not ideal, it simply results in objects appearing closer in the depth map than they actually are, and the distance from the cameras where an object is too close for the depth mapping algorithm to match it between the two images (the cameras are "cross-eyed" if you will) being extended.

\section{Image Pipeline}
\label{Subsection:comms}

As established in Chapter \ref{chapter:system}, the rover takes a picture with both cameras, abstracts those images, then sends that data off as a single combined packet to the server. While Chapter \ref{chapter:abstract} discussed the abstraction process as a single step that produces a full abstraction with both edges and spaces filled with colour, this does not reflect the implementation utilized in the full system. The final step of filling the spaces in the edge detected image using the selected seed points and average colours is completed on the server, not on the rover. Also, only one of the two images needs colour information at all, as the edges are the only part of the abstraction required to produce the server depth maps; the colours are to be used as part of a texture applied to the 3D model produced from the depth maps, therefore only one set is required. This results in the data packets being sent to the server consisting of two edge detected images and a set of seed points with corresponding colours for one of the images.

Attempts to implement this process on a single thread on the Pi, as tested successfully on a laptop, either crashed the Pi or would produce an unacceptable frame rate of around 1fps. To rectify this, both pipelining and parallelism were utilized to make better use of the Pi's quad core processor (Figure \ref{fig:threads}), and compromises were made in the quality of the abstractions to reduce the workload.

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=0.9\textwidth]{Figures/Threads.png}
      \caption[Raspberry Pi Image Pipeline Threading Block Diagram]{Raspberry Pi Image Pipeline Threading Block Diagram.}
      \label{fig:threads}
    \end{center}
\end{figure}

The capture thread handles signalling the cameras to capture and decoding the images. The capturing and decoding are done as separate operations in an effort to take the images as close to simultaneously as possible in a single thread. The first compromise in quality in favour of performance is made here, where the images are captured with a resolution of 320x240 rather than the cameras' standard resolution of 640x480. As will be discussed in greater depth below, the highest workload task in the pipeline is flood filling. Reducing the pixel count of every space by a factor of four therefore provides a significant improvement in performance. The smaller images also have the benefit of lower detail in high detail areas, so sections that would become areas of dense lines when edge detected (examples of this effect can be found in Appendix \ref{Appendix:demo}) are significantly less dense, containing less extraneous data.

The processing threads are concerned with the edge detection and compression of the images. The edge detection process is mostly as described in Chapter \ref{chapter:abstract}; the only difference is that only the image being sent from "Processing Thread 1" to the fill threads undergoes the final dilation step. The purpose of the dilation is to bridge gaps between the edges, creating defined shapes for the flood fill process. A side effect of dilation is a reduction in edge accuracy, which is very important when the images are to be used to produce depth maps later on. This leads to the logical conclusion that the images should be sent without dilation, and dilation only applied to find the colour data in the fill threads and then again on the server to create the complete coloured abstraction, allowing the depth maps to be constructed from non-dilated edges. 

While it can be induced from comparing the original images to their edge detected versions by eye that the latter contains less information than the former, this will only be reflected in real numbers if file format and compression are considered carefully. When the common image formats are compared, PNG would appear to be effective for this use case \cite{aguilera2006comparison}, as it excels at efficiently storing large blocks of the same colour (most of each edge detected image is black space). When tested on the edge detected images (Figure \ref{fig:compress}), it was confirmed that PNG provides the lowest file sizes, and compression level 5 provided the best ratio between file size and compression time (using the imencode function in OpenCV). More intelligent compression was tested using libimagequant \cite{libimagequant}, however it resulted in very poor performance (\textless2fps) with negligible improvement to the level of compression. An alternate option to compression as a bitmap was vectorising the images, however this was inaccurate and not as effective as bitmap compression (more information can be found in Appendix \ref{appendix:vectorization}).

\begin{figure}[H]
    \begin{center}
      \includegraphics[width=0.9\textwidth]{Figures/compression.png}
      \caption[Comparison of Image File Formats]{Comparison of Image File Formats. This data shows that PNG provides the smallest file sizes of the readily available file formats. It also shows that both standard JPG and PNG compression are very effective at reducing file size, however attempting to compress the JPG versions enough to compete with PNG would result in images of unusably poor quality (quality level 5-10).}
      \label{fig:compress}
    \end{center}
\end{figure}

The stage in the image pipeline that has the highest performance impact is the colour averaging, due to its use of flood filling. As the number of spaces being flood filled is determined by the number of points from the sobol sequence that hit unfilled spaces, the number of sobol points used is an important variable in tuning the performance of the system. In the testing done on a laptop in Chapter \ref{chapter:abstract}, the number of sobol points being used per image was between 400 and 600, and this filled a reasonable number of spaces while providing a reasonable frame rate. When this was attempted on the Pi however, the frame rate was below 1fps. This called for a significant reduction in sobol points and the application of parallelism. Good performance was achieved using 4 threads dedicated to the flood fill colour averaging task, each taking 50 sobol points. A total of 200 sobol points is a significant reduction from the laptop implementation of the algorithm, however it must also be noted that the images being used are much lower detail, so do not require a large number of sobol points to be mostly covered (Figure \ref{fig:LvsR}). 

\begin{figure}[H]
    \begin{center}
    \begin{tabular}{ c c }
        \includegraphics[width=0.45\textwidth]{Figures/abstractcompL.jpg} &
        \includegraphics[width=0.45\textwidth]{Figures/abstractcompR.jpg}
    \end{tabular}
    \caption[Comparison of Laptop and Rover Abstraction]{Comparison of Laptop and Rover Abstraction. The laptop abstraction (left) is high resolution, with only the smallest of spaces remaining unfilled such as the detail on the box stickers. The rover abstraction (right, this image being after the colour data and edge detected image had been combined on the server) is much lower resolution, with many more spaces remaining unfilled. However, it is still clearly the same scene with the geometry represented with almost the same level of accuracy, therefore making it an acceptable abstraction.}
    \label{fig:LvsR}
    \end{center}
\end{figure}

While it may seem as though the parallelization of this process could cause issues if two threads are attempting to fill the same space in the image simultaneously, this simply results in two successful seed points with slightly inaccurate average colours assigned to them, and only one of them being used to create the complete abstraction on the server. As the colours of the spaces are only recorded to provide the user a better understanding of the objects in the environment, it is not concerning if occasionally one is not an exact average.

Once the three pieces of data for an image pair (two compressed edge detected images and a set of colour data) have been generated, they must be combined into a single packet and sent to the server; this is covered by the transmission thread. While the images are at this stage already compressed into a set of bytes that can be transmitted as is, the colour data needs its own custom packaging. Each seed point-average colour pair is formed into its own sub-packet with the structure shown in Table \ref{table:colour}. These sub-packets make up the colour data data segment within the complete data packet.

\begin{table}[H]
\centering
\caption{Colour Data Sub-Packet Structure. The seed point components are given 2 bytes each due to their maximum values being larger than a single byte can store.}
\label{table:colour}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Byte in Colour Data Sub-Packet & 1          & 2         & 3          & 4         & 5         & 6           & 7         \\ \hline
Usage                          & \multicolumn{4}{c|}{Seed Point}                 & \multicolumn{3}{c|}{Average Colour} \\ \hline
Component                      & \multicolumn{2}{c|}{X} & \multicolumn{2}{c|}{Y} & Red       & Green       & Blue      \\ \hline
\end{tabular}
\end{table}

As the size of each data segment is unknown and highly variable, knowing where one ends and another begins on the server is a challenge. The solution utilized in this instance is separating each data set with a splitter made up of 7 bytes that contain 0, 1, 2, 3, 4, 5, and 6. This sequence of bytes is extremely unlikely to occur within the data, so can be easily used to pinpoint the starts and ends of the data sets. This therefore leads to the data packet format shown in Table \ref{table:packet}.

\begin{table}[H]
\centering
\caption{Data Packet Format.}
\label{table:packet}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Data Segment & \multicolumn{3}{c|}{Colour Data}  & Splitter & Img 1          & Splitter & Img 2          \\ \hline
Contents     & Sub-Packet 1 & Sub-Packet 2 & ... & 0...6    & PNG Data & 0...6    & PNG Data \\ \hline
\end{tabular}}
\end{table}

\section{Control System}

The primary aim when designing the control system was to make controlling the rover easy and intuitive; the user must have no trouble understanding the controls whether in or out of VR. The Xbox 360 controller can be integrated into almost any system, is intuitive for even non-gamers to operate, and can be used both with and without a VR headset, leading it to be selected as the control input method.

The motion of the rover is restricted to forwards or backwards with variable speed, or rotation with a fixed speed. This is so the rover is forced to only rotate in short intervals, as rotation causes a reduction in depth map accuracy that forward and backwards motion does not (the extent and causes of this will be discussed in Chapter \ref{chapter:eval}). The user has the ability to chose between full and half speed drive modes, to provide the flexibility of both being able to run the motors at full power in one mode and drive at low speeds with greater control in the other. Due to the gimbal not having a full range of motion (it can rotate roughly 200 degrees before it starts to pull its own cabling out), its range must be restricted in software. This is done by keeping track of the number of steps the stepper motors have undertaken as an estimator of the gimbal's orientation and preventing it from moving past certain angles. This system is effective, however the limits drift over time due to the stepper motors not always moving through the exact same number of steps requested by the control system. This drift, while not ideal, is only an issue over long activation periods, so is not a major concern for this project. The final element the user has control over is the low threshold in the edge detection applied as part of the data abstraction (this is the single point of contact between the image pipeline and control system that is mentioned in Chapter \ref{chapter:system}). Changing the low threshold of the edge detection changes the amount of detail in the 3D model, having a major impact on its quality. Additionally, the threshold level that produces the optimal level of 3D model detail changes depending on the space the rover is observing, therefore it is essential that the user has control of this at run-time.

The user inputs are transmitted to the rover over UDP, each packet with the structure shown in Table \ref{table:control}. The packets are sent at a constant rate of 50 per second, even when the user is not inputting any commands. This is so that in the event that the wireless link is failing, the inconsistency of the constant message stream will notify the rover that there is an issue. When the rover detects that it has been an unreasonable amount of time since the last message, it stops all its motors until it starts receiving messages again, to prevent it from damaging itself while the user has lost control.

\begin{table}[H]
\centering
\caption{Control Data Packet Structure. The analogue stick axes are mapped to between 0 and 255 to fit them within a byte each, as greater accuracy than that was not required. The low threshold only being alocated a byte means it is also capped at 255, however through testing it has been determined that values even above 100 produce images with no edges anyway, so this limit is not a concern.}
\label{table:control}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Byte in Control Packet & 0                                                 & 1                                                & 2               & 3                                                 & 4                                                & 5          \\ \hline
Usage                  & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Driving Control\\ (Left Analogue Stick)\end{tabular}} & Low Threshold   & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Gimbal Control\\ (Right Analogue Stick)\end{tabular}} & Drive Mode \\ \hline
Contents               & X-axis                                            & Y-axis                                           & Threshold Value & X-axis                                            & Y-axis                                           & 0/1        \\ \hline
\end{tabular}}
\end{table}